{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eashanair05/majorproject/blob/main/Copy_of_bhuwan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-E2MjMtb4qp",
        "outputId": "f3588ea3-be71-4809-9bb2-11fa746da74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/image_dataset.zip, /content/drive/MyDrive/image_dataset.zip.zip or /content/drive/MyDrive/image_dataset.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/image_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "utwyjpP_J7Vq",
        "outputId": "0156d558-d696-4ba2-d38e-b895a67fc5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNrDFkSScoiv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import model_selection\n",
        "\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision.utils import make_grid\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjeZaha9f_d8"
      },
      "outputs": [],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXbCWeb8codq"
      },
      "outputs": [],
      "source": [
        "ROOT_PATH = os.getcwd()\n",
        "BASE_PATH = os.path.join(ROOT_PATH, '2750')\n",
        "DATA_PATH = os.path.join(ROOT_PATH, 'Dataset')\n",
        "FULL_DATA_DF = os.path.join(ROOT_PATH, 'FULL_DATA.csv')\n",
        "\n",
        "if not os.path.isdir(DATA_PATH):\n",
        "    os.mkdir(DATA_PATH)\n",
        "\n",
        "IDX_CLASS_LABELS = {\n",
        "    0: 'AnnualCrop',\n",
        "    1: 'Forest', \n",
        "    2: 'HerbaceousVegetation',\n",
        "    3: 'Highway',\n",
        "    4: 'Industrial',\n",
        "    5: 'Pasture',\n",
        "    6: 'PermanentCrop',\n",
        "    7: 'Residential',\n",
        "    8: 'River',\n",
        "    9: 'SeaLake'\n",
        "}\n",
        "CLASSES = ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture','PermanentCrop','Residential','River', 'SeaLake']\n",
        "CLASS_IDX_LABELS = dict()\n",
        "for key, val in IDX_CLASS_LABELS.items():\n",
        "    CLASS_IDX_LABELS[val] = key\n",
        "\n",
        "NUM_CLASSES = len(IDX_CLASS_LABELS.items())\n",
        "torch.manual_seed(10)\n",
        "VALID_SIZE = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2Vp6QZJcobk"
      },
      "outputs": [],
      "source": [
        "ROOT_PATH\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA4tSvQRcoZX"
      },
      "outputs": [],
      "source": [
        "CLASS_IDX_LABELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDAeYuf7coXI"
      },
      "outputs": [],
      "source": [
        "## Give idx of each class name\n",
        "def encode_label(label):\n",
        "  idx = CLASS_IDX_LABELS[label] \n",
        "  return idx\n",
        "\n",
        "## Take in idx and return the class name\n",
        "def decode_target(target, text_labels=True):\n",
        "  result = []\n",
        "  if text_labels:\n",
        "    return IDX_CLASS_LABELS[target]\n",
        "  else:\n",
        "    return target\n",
        "\n",
        "## Show batches of images\n",
        "def show_batch(dl):\n",
        "  for images, labels in dl:\n",
        "    fig, ax = plt.subplots(figsize=(16, 8))\n",
        "    ax.set_xticks([]); ax.set_yticks([])\n",
        "    ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqd7sXlRcoVC"
      },
      "outputs": [],
      "source": [
        "## Example for decoding and encoding\n",
        "print(encode_label('Forest'), decode_target(2))\n",
        "print(decode_target(2, text_labels=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEIk8g8scoSp"
      },
      "outputs": [],
      "source": [
        "from os import walk\n",
        "i = 0\n",
        "for (dirpath, dirname, filename) in walk(BASE_PATH):\n",
        "  print(\"Directory Path: \", dirpath)\n",
        "  print(\"Directory Name: \", dirname)\n",
        "  print(\"Filename : \", filename)\n",
        "  print(\"----------------------\"*10)\n",
        "  i += 1\n",
        "  if i > 1:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHvdS9OecoQU"
      },
      "outputs": [],
      "source": [
        "from os import walk\n",
        "\n",
        "i = 0\n",
        "DATA_DF = pd.DataFrame(columns = ['image_id', 'label'], dtype=object) \n",
        "\n",
        "for (dirpath, dirname, filename) in walk(BASE_PATH):\n",
        "  for each_file in filename:\n",
        "    DATA_DF.loc[i] = [each_file, dirpath.split('/')[-1]]\n",
        "    i += 1\n",
        "    # break\n",
        "DATA_DF.to_csv(FULL_DATA_DF, index=False)\n",
        "DATA_DF.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dtn9oC0vcoN5"
      },
      "outputs": [],
      "source": [
        "DATA_DF = pd.read_csv(FULL_DATA_DF)\n",
        "DATA_DF = DATA_DF.sample(frac = 1, random_state=48) \n",
        "TRAIN_DF = DATA_DF[:-int(len(DATA_DF)*VALID_SIZE)]\n",
        "VALID_DF = DATA_DF[-int(len(DATA_DF)*VALID_SIZE) :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4CiNgsPcoLq"
      },
      "outputs": [],
      "source": [
        "TRAIN_DF.reset_index(inplace = True) \n",
        "TRAIN_DF.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4HKF9iecoJT"
      },
      "outputs": [],
      "source": [
        "VALID_DF.reset_index(inplace = True) \n",
        "VALID_DF.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbmLI3HrcoGu"
      },
      "outputs": [],
      "source": [
        "TRAIN_DF.size, VALID_DF.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i76gqgRRcoES"
      },
      "outputs": [],
      "source": [
        "class EuroSAT(Dataset):\n",
        "  def __init__(self, train_df, train_dir, transform=None):\n",
        "    self.train_dir = train_dir\n",
        "    self.train_df = train_df\n",
        "    self.transform = transform\n",
        "        \n",
        "  def __len__(self):\n",
        "    return len(self.train_df)\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    row = self.train_df.loc[idx]\n",
        "    img_id, label = row['image_id'], row['label']\n",
        "    img = Image.open(os.path.join(self.train_dir, img_id.split('.')[0].split('_')[0], img_id))\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    return img, encode_label(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5VgiKNncn-5"
      },
      "outputs": [],
      "source": [
        "## Dataset and transformations\n",
        "data_transform = transforms.Compose([\n",
        "                                transforms.Resize(size=(224, 224)),\n",
        "#                                 transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor(),\n",
        "                                # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                ])\n",
        "train_ds = EuroSAT(TRAIN_DF, BASE_PATH, data_transform)\n",
        "valid_ds = EuroSAT(VALID_DF, BASE_PATH, data_transform)\n",
        "print(len(train_ds), len(valid_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7dN64Obcn1e"
      },
      "outputs": [],
      "source": [
        "## Data loaders and showing batch of data\n",
        "batch_size = 64\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "show_batch(train_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VATOP3IfhVvI"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exhT-UKohVBR"
      },
      "outputs": [],
      "source": [
        "def accuracy(outputs, labels):\n",
        "  _, preds = torch.max(outputs, dim = 1)\n",
        "  return torch.tensor(torch.sum(preds==labels).item() / len(preds))\n",
        "\n",
        "class MulticlassClassifierBase(nn.Module):\n",
        "    \n",
        "  def training_step(self, batch):\n",
        "    img, label = batch\n",
        "    out = self(img)\n",
        "    loss = criterion(out, label)\n",
        "    accu = accuracy(out, label)\n",
        "    return accu ,loss\n",
        "  def validation_step(self, batch):\n",
        "    img, label = batch\n",
        "    out = self(img)\n",
        "    loss = criterion(out, label)\n",
        "    accu = accuracy(out, label)\n",
        "    return {\"val_loss\": loss.detach(), \"val_acc\": accu}\n",
        "  \n",
        "  def validation_epoch_ends(self, outputs):\n",
        "    batch_loss = [x['val_loss'] for x in outputs]\n",
        "    epoch_loss = torch.stack(batch_loss).mean()\n",
        "    batch_acc = [x['val_acc'] for x in outputs]\n",
        "    epoch_acc = torch.stack(batch_acc).mean()\n",
        "    return {\"val_loss\":epoch_loss.item(), \"val_acc\":epoch_acc.item()}\n",
        "  def epoch_end(self, epoch, result):\n",
        "    print(\"Epoch [{}],train_accu: {:.4f}, learning_rate: {:.4f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "        epoch,result['train_accu'], result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWvYwkpyhU_K"
      },
      "outputs": [],
      "source": [
        "class LULC_Model(MulticlassClassifierBase):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.network = models.wide_resnet50_2(pretrained=True)\n",
        "    n_inputs = self.network.fc.in_features\n",
        "    self.network.fc = nn.Sequential(\n",
        "                          nn.Linear(n_inputs, 256),\n",
        "                          nn.ReLU(),\n",
        "                          nn.Dropout(0.5),\n",
        "                          nn.Linear(256, NUM_CLASSES),\n",
        "                          nn.LogSoftmax(dim=1)\n",
        "                            )\n",
        "  def forward(self, xb):\n",
        "    return self.network(xb)\n",
        "    \n",
        "  def freeze(self):\n",
        "    for param in self.network.parameters():\n",
        "      param.require_grad=False\n",
        "      for param in self.network.fc.parameters():\n",
        "        param.require_grad=True\n",
        "  def unfreeze(self):\n",
        "    for param in self.network.parameters():\n",
        "      param.require_grad=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQg-Fjn-hU9Q"
      },
      "outputs": [],
      "source": [
        "model = LULC_Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0mU7n7rhU7m"
      },
      "outputs": [],
      "source": [
        "## Running through the data loader => Model => Output\n",
        "def try_batch(dl):\n",
        "  for images, labels in dl:  \n",
        "    print(images.shape)\n",
        "    out = model(images)\n",
        "    print(out.shape)\n",
        "    print(out[0])\n",
        "    break\n",
        "try_batch(train_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCl4LciVhzDJ"
      },
      "source": [
        "## Training and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IP60XcWhU4z"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, valid_loader):\n",
        "  model.eval()\n",
        "  outputs = [model.validation_step(batch) for batch in valid_loader]\n",
        "  return model.validation_epoch_ends(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    return param_group['lr']\n",
        "    \n",
        "def fit(epochs, max_lr,  model, train_loader, valid_loader, weight_decay=0,\\\n",
        "                grad_clip=None,opt_func=torch.optim.SGD, max_epochs_stop=3):\n",
        "  \n",
        "  history = []\n",
        "  valid_loss_min = np.Inf\n",
        "  valid_acc_max = 0\n",
        "  model_file_name = 'lulc.pth'\n",
        "  model_file_name2 = 'lulc_max_acc.pth'\n",
        "  epochs_no_improve =  0\n",
        "  optimizer = opt_func(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.00001)\n",
        "                         \n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    train_accu = []\n",
        "    lrs = []\n",
        "    for batch in tqdm(train_loader):\n",
        "        \n",
        "      accu, loss = model.training_step(batch)\n",
        "      train_loss.append(loss)\n",
        "      train_accu.append(accu)\n",
        "      loss.backward()\n",
        "        ## Gradient Clipping\n",
        "      if grad_clip:\n",
        "        nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "        \n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "        \n",
        "      lrs.append(get_lr(optimizer))\n",
        "        \n",
        "        \n",
        "        \n",
        "    result = evaluate(model, valid_loader)\n",
        "    scheduler.step(result['val_loss'])\n",
        "    ########### Early Stopping ##############                                         \n",
        "    valid_loss = result['val_loss']\n",
        "    valid_acc = result['val_acc']\n",
        "    if valid_acc > valid_acc_max:\n",
        "      torch.save(model.state_dict(), model_file_name2)\n",
        "      valid_acc_max = valid_acc\n",
        "    if valid_loss<valid_loss_min:\n",
        "      torch.save(model.state_dict(), model_file_name)\n",
        "      valid_loss_min = valid_loss                                  \n",
        "      epochs_no_improve = 0          \n",
        "    else:\n",
        "      epochs_no_improve += 1\n",
        "      if epochs_no_improve > max_epochs_stop:\n",
        "        result[\"train_loss\"] = torch.stack(train_loss).mean().item()\n",
        "        result[\"train_accu\"] = torch.stack(train_accu).mean().item()\n",
        "        result[\"lrs\"] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "        print(\"Early Stopping............................\")\n",
        "        return history                                \n",
        "                                              \n",
        "    result[\"train_loss\"] = torch.stack(train_loss).mean().item()\n",
        "    result[\"train_accu\"] = torch.stack(train_accu).mean().item()\n",
        "    result[\"lrs\"] = lrs\n",
        "    model.epoch_end(epoch, result)\n",
        "    history.append(result)\n",
        "  print(\"VAL LOSS MIN {}\".format(valid_loss_min))\n",
        "  print(\"VAL ACC MAX {}\".format(valid_acc_max))\n",
        "  return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6uGwfCEhU2p"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "  print(torch.cuda.is_available())\n",
        "  if torch.cuda.is_available():\n",
        "    return torch.device('cuda')\n",
        "  else:\n",
        "    return torch.device('cpu')\n",
        "def to_device(data, device):\n",
        "  if isinstance(data, (list, tuple)):\n",
        "    return [to_device(x, device) for x in data]\n",
        "  return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "  def __init__(self, dl, device):\n",
        "    self.dl = dl\n",
        "    self.device = device\n",
        "        \n",
        "  def __iter__(self):\n",
        "    for b in self.dl:\n",
        "      yield to_device(b, self.device)\n",
        "            \n",
        "  def __len__(self):\n",
        "    return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc4xHY2ihU0a"
      },
      "outputs": [],
      "source": [
        "device = get_device()\n",
        "## Loading data to devide\n",
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "valid_dl = DeviceDataLoader(valid_dl, device)\n",
        "## Loading model to device\n",
        "model = to_device(LULC_Model(), device)\n",
        "## lets try passing a batch to model again\n",
        "try_batch(train_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL9-x3WghUyM"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz3fDhEYkN5K"
      },
      "outputs": [],
      "source": [
        "# import gc\n",
        "# del variables\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEgBhBLXjKLm"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eQ8EzAShUv-"
      },
      "outputs": [],
      "source": [
        "## Hyper Parameters\n",
        "max_epochs_stop = 11\n",
        "max_lr = 1e-4\n",
        "grad_clip = 0.1\n",
        "weight_decay = 1e-3\n",
        "batch_size = 64\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 10\n",
        "opt_func = torch.optim.Adam\n",
        "## Evaluating with non-trained model\n",
        "evaluate(model, valid_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoQQJSShhUtn"
      },
      "outputs": [],
      "source": [
        "## Freezing except last layer\n",
        "model.freeze()\n",
        "## Training\n",
        "history = fit(epochs, max_lr, model, train_dl, valid_dl, weight_decay, grad_clip, opt_func, max_epochs_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikwsSoFmjVYu"
      },
      "source": [
        "# Reports\n",
        "### Training Reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL5wv-hFhUrH"
      },
      "outputs": [],
      "source": [
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, '-bx')\n",
        "    plt.plot(val_losses, '-rx')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. No. of epochs');\n",
        "    \n",
        "plot_losses(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2S2tthBrqnY"
      },
      "outputs": [],
      "source": [
        "def plot_accuracy(history):\n",
        "        \n",
        "    train_accu = [x.get('train_accu') for x in history]\n",
        "    val_accu = [x['val_acc'] for x in history]\n",
        "    plt.plot(train_accu, '-bx')\n",
        "    plt.plot(val_accu, '-rx')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Accuracy vs. No. of epochs');\n",
        "plot_accuracy(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M23RkQ6Gr2t4"
      },
      "outputs": [],
      "source": [
        "def plot_lrs(history):\n",
        "    lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
        "    plt.plot(lrs)\n",
        "    plt.xlabel('Batch no.')\n",
        "    plt.ylabel('Learning rate')\n",
        "    plt.title('Learning Rate vs. Batch no.');\n",
        "    \n",
        "plot_lrs(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lro_l9Hbr4z0"
      },
      "outputs": [],
      "source": [
        "def show_sample(img, target=None):\n",
        "    if target:\n",
        "        print(\"Label\" ,decode_target(int(target), text_labels=True))\n",
        "    plt.imshow(img.permute(1, 2, 0))\n",
        "\n",
        "### Predict Single Images\n",
        "def predict_single(image):\n",
        "    show_sample(image)\n",
        "    xb = image.unsqueeze(0)\n",
        "    xb = to_device(xb, device)\n",
        "    preds = model(xb)\n",
        "    _, prediction = torch.max(preds.cpu().detach(), dim=1)\n",
        "    return decode_target(int(prediction), text_labels=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlLSyxSTsMJU"
      },
      "outputs": [],
      "source": [
        "predict_single(valid_ds[20][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfpsWGmGscKh"
      },
      "outputs": [],
      "source": [
        "predict_single(valid_ds[30][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahzMOQTVsgF2"
      },
      "outputs": [],
      "source": [
        "predict_single(valid_ds[90][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxEW07PWsmhL"
      },
      "outputs": [],
      "source": [
        "batch_size =1\n",
        "@torch.no_grad()\n",
        "def predict_dl(dl, model):\n",
        "    torch.cuda.empty_cache()\n",
        "    batch_pred, labels = [], []\n",
        "    for xb, label in tqdm(dl):\n",
        "        probs = model(xb)\n",
        "        _, pred = torch.max(probs.cpu().detach(), dim=1)\n",
        "        batch_pred.append(pred.cpu().detach())     \n",
        "        labels.append(label)   \n",
        "    return [x for x in batch_pred], \\\n",
        "            [x for x in labels]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}